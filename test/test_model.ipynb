{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import os\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, AutoModelForSeq2SeqLM\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from scipy.spatial import distance\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "emo_model_id = \"SamLowe/roberta-base-go_emotions\"\n",
    "emo_classifier = pipeline('text-classification', model=emo_model_id,tokenizer=emo_model_id, max_length=512, truncation=True, top_k=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING data emotion probability distribution...\n"
     ]
    }
   ],
   "source": [
    "if(os.path.exists('../modeldata/emo_probi.p')):\n",
    "    print(\"LOADING data emotion probability distribution...\")\n",
    "    with open('../modeldata/emo_probi.p', \"rb\") as f:\n",
    "        [all_emo_probi, _] = pickle.load(f)\n",
    "    f.close()\n",
    "all_emo_probi = dict(all_emo_probi)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def append_scores(labels, original, sample):\n",
    "    all_emo_scores = original\n",
    "    for sam in sample:\n",
    "        for s in sam:\n",
    "            emo = s.get('label')\n",
    "            prev_score = original[emo]\n",
    "            score = s.get('score')\n",
    "            all_emo_scores[emo] = (prev_score + score)\n",
    "    all_scores = list(zip(*all_emo_scores.items()))[1]\n",
    "    probi = softmax(all_scores)\n",
    "    all_emo_scores = dict(zip(labels, probi))\n",
    "    return all_emo_scores\n",
    "\n",
    "def weighted_bleu_score(target, response):\n",
    "    score1 = nltk.translate.bleu_score.sentence_bleu([target], response, weights=(1, 0, 0))\n",
    "    score2 = nltk.translate.bleu_score.sentence_bleu([target], response, weights=(0, 1, 0))\n",
    "    score3 = nltk.translate.bleu_score.sentence_bleu([target], response, weights=(0, 0, 1))\n",
    "    ngram_score_list = [score1, score2, score3]\n",
    "    return (sum(ngram_score_list) / len(ngram_score_list))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../DEV-blenderbot-400m-emo-probi-bleu-epoch0-score0.382-bleu0.06629 were not used when initializing BlenderbotForConditionalGeneration: ['v_head.summary.weight', 'v_head.summary.bias']\n",
      "- This IS expected if you are initializing BlenderbotForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BlenderbotForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "print(\"Start testing.\")\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\"\n",
    "BLEU_score_list = []\n",
    "texts = []\n",
    "ppo_model = \"../DEV-blenderbot-400m-emo-probi-bleu-epoch0-score0.382-bleu0.06629\"\n",
    "model_id = ppo_model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map={\"\": device}, torch_dtype=torch.float32)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/10 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cdbe2849778b4a549affb71256a7bcce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"../modeldata/test_dialogue_dataset.p\", \"rb\") as f:\n",
    "    [test_dataset] = pickle.load(f)\n",
    "test_dataset = Dataset.from_dict(test_dataset)[:10]\n",
    "test_dataset = Dataset.from_dict(test_dataset)\n",
    "\n",
    "def tokenize(sample):\n",
    "    prompt = sample[\"prompt\"] # prompt\n",
    "    continuation = sample[\"target\"] # utterance\n",
    "\n",
    "    sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
    "    #sample[\"target_ids\"] = tokenizer.encode(continuation)[: input_size()]\n",
    "    sample[\"query\"] = {\"prompt\": tokenizer.decode(sample[\"input_ids\"]), \"target\": continuation}\n",
    "    return sample\n",
    "\n",
    "test_dataset = test_dataset.map(tokenize, batched=False)\n",
    "test_dataset.set_format(type=\"torch\")\n",
    "emp_weight = 0.2\n",
    "fluency_weight = 0.8"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing...\n",
      "10\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "dict_values([0.015353312715888023, 0.004193090833723545, 0.003757640952244401, 0.007810292299836874, 0.032693423330783844, 0.008610889315605164, 0.0044470117427408695, 0.00202195905148983, 0.004049321636557579, 0.017276519909501076, 0.007352734915912151, 0.011946394108235836, 0.004603976849466562, 0.005533120594918728, 0.817112147808075, 0.002720079617574811, 0.011198784224689007, 0.008417824283242226, 0.0030771277379244566, 0.040655042976140976, 0.09969515353441238, 0.010760837234556675, 0.0029458615463227034, 0.04813799634575844, 0.006577658466994762, 0.002215614775195718, 0.07298275083303452, 0.012798329815268517])\n",
      "[('admiration', 0.041058462113142014), ('amusement', 0.012828965671360493), ('anger', 0.005675230175256729), ('annoyance', 0.008834452368319035), ('approval', 0.039628565311431885), ('caring', 0.15474122762680054), ('confusion', 0.15657512843608856), ('curiosity', 0.17108076810836792), ('desire', 0.006378207355737686), ('disappointment', 0.00921199843287468), ('disapproval', 0.012249909341335297), ('disgust', 0.004163969773799181), ('embarrassment', 0.005383211653679609), ('excitement', 0.04536726325750351), ('fear', 0.6555145382881165), ('gratitude', 0.013474688865244389), ('grief', 0.015703465789556503), ('joy', 0.17957666516304016), ('love', 0.012920156121253967), ('nervousness', 0.3990369737148285), ('neutral', 0.02051645889878273), ('optimism', 0.024995282292366028), ('pride', 0.0042824335396289825), ('realization', 0.027446230873465538), ('relief', 0.035290516912937164), ('remorse', 0.006800961215049028), ('sadness', 0.0663459375500679), ('surprise', 0.020687159150838852)]\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nzeros = [0] * len(labels)\\nscore_dict = dict(zip(labels, zeros))\\nempathy_results = append_scores(labels, score_dict, emo_results)\\n# sort alphabetically\\nempathy_results = dict(sorted(empathy_results.items(), key=lambda x: x[0].lower()))\\nall_emo_probi_values = list(all_emo_probi.values())\\nempathy_results_values = list(empathy_results.values())\\n\\njs_distance = distance.jensenshannon(all_emo_probi_values, empathy_results_values)\\n# js_distance: identical = 0, entirely different = 1, reverse this for reward\\nemo_score = 1 - js_distance\\n\\ncurrent_score = (emo_score * emp_weight) + (mean_bleu * fluency_weight)\\nwith open(f\\'DEV_test_score_log_emo_probi_score.txt\\', \\'wb\\') as score_log:\\n    score_log.write(f\"Mean BLEU of this model: {mean_bleu}. \\n\")\\n    score_log.write(f\"Emo distribution similarity of this model: {emo_score}. \\n\")\\n    score_log.write(f\"Score of this model: {current_score}. \\n\")\\n    print(f\"Mean BLEU of this model: {mean_bleu}. \\n\")\\n    print(f\"Emo distribution similarity of this model: {emo_score}. \\n\")\\n    print(f\"Score of this model: {current_score}. \\n\")\\nscore_log.close()\\n#except Exception as err:\\n#    with open(f\\'DEV_test_error_log_emo_probi_score.txt\\', \\'w\\') as err_log:\\n#        err_log.write(f\"Unexpected {err=}, {type(err)=}\")\\n#    err_log.close()\\n'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Start testing...\")\n",
    "print(len(test_dataset))\n",
    "#try:\n",
    "with open(f'DEV_test_text_log_emo_probi_score.txt', 'w') as text_log:\n",
    "    counter = 0\n",
    "    prompts = []\n",
    "    for test_query in test_dataset:\n",
    "        input_texts = test_query[\"prompt\"]\n",
    "        prompts.append(input_texts)\n",
    "        target = test_query[\"query\"][\"target\"]\n",
    "        #print(target)\n",
    "        input_ids = tokenizer(input_texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        #input_ids = test_query['input_ids']\n",
    "        outputs = model.generate(**input_ids, do_sample=True, max_new_tokens=40, use_cache=True)\n",
    "        generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        texts.append(generated_texts[0])\n",
    "        #text_log.write(f\"{counter} Prompt: {input_texts} \\n\")\n",
    "        #text_log.write(f\"{counter} Response: {generated_texts[0]} \\n\")\n",
    "        #text_log.write(f\"{counter} Ref: {target} \\n\")\n",
    "        counter += 1\n",
    "        print(counter)\n",
    "\n",
    "        # Calculate bleu score\n",
    "        test_response = word_tokenize(generated_texts[0])\n",
    "        dev_target = word_tokenize(test_query[\"target\"])\n",
    "        dev_BLEUscore = weighted_bleu_score(dev_target, test_response)\n",
    "        BLEU_score_list.append(dev_BLEUscore)\n",
    "text_log.close()\n",
    "\n",
    "mean_bleu = sum(BLEU_score_list) / len(BLEU_score_list)\n",
    "\n",
    "# calculate emo distribution\n",
    "prompt_results = emo_classifier(prompts)\n",
    "emo_results = emo_classifier(texts)\n",
    "labels = [s.get('label') for s in emo_results[0]]\n",
    "zeros = [0] * len(labels)\n",
    "list_js_distance = []\n",
    "for i in range(len(prompt_results)):\n",
    "    #print(prompt_results[i])\n",
    "    #print(emo_results[i])\n",
    "    prompt_dict = dict(zip(labels, zeros))\n",
    "    response_dict = dict(zip(labels, zeros))\n",
    "    for j in range(len(prompt_results[i])):\n",
    "        label = prompt_results[i][j].get(\"label\")\n",
    "        prompt_dict[label] = prompt_results[i][j].get(\"score\")\n",
    "        label = emo_results[i][j].get(\"label\")\n",
    "        response_dict[label] = emo_results[i][j].get(\"score\")\n",
    "\n",
    "    prompt_value = dict(sorted(prompt_dict.items(), key=lambda x: x[0].lower())).values()\n",
    "    prompt_value = list(prompt_value)\n",
    "    #print(prompt_value)\n",
    "    response_value = dict(sorted(response_dict.items(), key=lambda x: x[0].lower())).values()\n",
    "    response_value = list(response_value)\n",
    "\n",
    "    js_distance = distance.jensenshannon(prompt_value, response_value)\n",
    "    list_js_distance.append(js_distance)\n",
    "    #print(response_value)\n",
    "mean_js_distance = sum(list_js_distance) / len(list_js_distance)\n",
    "\n",
    "\"\"\"\n",
    "zeros = [0] * len(labels)\n",
    "score_dict = dict(zip(labels, zeros))\n",
    "empathy_results = append_scores(labels, score_dict, emo_results)\n",
    "# sort alphabetically\n",
    "empathy_results = dict(sorted(empathy_results.items(), key=lambda x: x[0].lower()))\n",
    "all_emo_probi_values = list(all_emo_probi.values())\n",
    "empathy_results_values = list(empathy_results.values())\n",
    "\n",
    "js_distance = distance.jensenshannon(all_emo_probi_values, empathy_results_values)\n",
    "# js_distance: identical = 0, entirely different = 1, reverse this for reward\n",
    "emo_score = 1 - js_distance\n",
    "\n",
    "current_score = (emo_score * emp_weight) + (mean_bleu * fluency_weight)\n",
    "with open(f'DEV_test_score_log_emo_probi_score.txt', 'wb') as score_log:\n",
    "    score_log.write(f\"Mean BLEU of this model: {mean_bleu}. \\n\")\n",
    "    score_log.write(f\"Emo distribution similarity of this model: {emo_score}. \\n\")\n",
    "    score_log.write(f\"Score of this model: {current_score}. \\n\")\n",
    "    print(f\"Mean BLEU of this model: {mean_bleu}. \\n\")\n",
    "    print(f\"Emo distribution similarity of this model: {emo_score}. \\n\")\n",
    "    print(f\"Score of this model: {current_score}. \\n\")\n",
    "score_log.close()\n",
    "#except Exception as err:\n",
    "#    with open(f'DEV_test_error_log_emo_probi_score.txt', 'w') as err_log:\n",
    "#        err_log.write(f\"Unexpected {err=}, {type(err)=}\")\n",
    "#    err_log.close()\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
